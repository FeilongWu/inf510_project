1. The names of team member(s) 
Feilong Wu

2. How to run your code (what command-line switches they are, what happens when you invoke the code, etc.)
This program requires packages including pandas, sqlite3, matplotlib, and numpy (requirements.txt, sqlcm.py, and main.py should be placed in the same folder of the notebook). Clone the repository with HTTPS from GitHub at https://github.com/FeilongWu/inf510_project. If unsure about package installations, one can install the packages with this command:
$ pip install --user --requirement requirements.txt
To run the code in notebook, one should execute the code in each cell to display the analysis results. An alternative to obtain the analysis results is to run main.py in command line with Python. Once it has been invoked, the program will display the results of data analysis in the forms of text and graph to answer the questions proposed for this project.

3. Any major “gotchas” to the code (i.e. things that don’t work, go slowly, could be improved, etc.)
Loading libraries may take a while. Variable names like "func" and "table_name" are overwritten several times in different context of data analysis (i.e. "table_name" has different values in processing income vs birth and income vs unemployment rate). The last figure is a surface plot, for which the axes for income per capita and median home price do not share the same origins. 

4. Anything else you feel is relevant to the grading of your project your project. 
The database for the final submission is slightly different from that in milestone 2. After realizing that the data of unemployment rate in 2013 is required for data analysis, the new database is extended to include that data and everything else is the same. The code can be run in both notebook and command line using the wu_feilong.ipynb and main.py, respectively. 

5. What did you set out to study?  (i.e. what was the point of your project?  This should be close to your Milestone 1 assignment, but if you switched gears or changed things, note it here.) 
This project is about demographics of US counties. Similar to the questions proposed in milestone stone 1, the final project answers the following four questions. The first is to find the five counties with the highest birth rate. The second and third questions are asking the correlations between income per capita and two variables birth rate and unemployment rate, respectively. The last question is about how both income per capita and home price contribute to birth rate. 

6. What did you Discover/what were your conclusions (i.e. what were your findings?  Were your original assumptions confirmed, etc.?) 
The counties with the highest birth rate appear to have low population density, especially the one in Alaska. The is a normal pattern observed in nature that a large number of living organisms in a bounded region tend to grow slowly as the increasing competition for food, water, and living space keeps them in check. 
The correlation between income per capita and birth rate is low despite whether their values are normalized or not (only the normalized scatter plot is shown), so the income may not affect the decision for a person to have a child too much. What is surprising is that the correlation coefficient has a negative sign, for which I expected to be positive since people with higher income may be able to afford the cost for rising more children and therefore would like to have more children. In contrast, the unemployment rate is a good predictor for income per capita due to their relatively high correlation coefficient. Moreover, their correlation coefficient is negative and reasonable that counties with high income per capita are more economically advantageous and therefore lower in unemployment rate.
Generally, the counties with higher home price tend to have higher birth rate and vice versa. As mentioned above, the birth rate drops when income per capita increases, however, it drops faster in those counties with lower home price than those with higher home price.

7. What difficulties did you have in completing the project?   
Collecting data is not easy. Although data is abundant given search results, the data source that can satisfy the requirements is not much. The data should be accessible and come from stable sources including institutions and government agencies. Another requirement for data is that data is that it should allow web scrapping or API queries, which is not possible in an interactive table (not encoded in a markup language) or a PDF page (encoded in machine-readable unicode which cannot be decoded to string easily).  It was difficult to obtain the income per capita from 2016 to 2018 from a CSV file as the data was stored in a data table rather than in cells. Unicode is the result of reading the data in traditional ways, which cannot be converted to human-readable strings after many attempts. I finally converted read the data table into a data sheet structure by using an external library. Writing the cleaning data into the database is another difficulty due to the diverse raw data formats and uniform database format. For example, the raw data can have different columns of incomes for different years, but the database table for income has only one column for all incomes and another column for specifying the corresponding year.  In data analysis, data integration is also difficult. Variables like birth rate is stored in database that a column contains all the values of the feature, but data analysis requires a data frame in which multiple columns contain the same attribute in different years and merge with another feature like income. During this process of data extraction from database and merging data frames, NaN can appear when a county has no value for a particular year. Although NaN can exist for a record, that record should be used in calculating correlation coefficient as long as it has real values for all features of interest in other years rather than the one with NaN.  

8. What skills did you wish you had while you were doing the project? 
I wish I were able to process unicode. For instance, I could extract a data table from a URL for a PDF opened in a web page. Since the performance of SQL database for big data may be a concern, I wish I could use SQLAlchemy toolkit for constructing and querying database tables. In data visualization, graphs are plotted by the library "matplotlib" and rendered by windows. This is may not be attractive since the command line is in a session and cannot receive any input, when the windows are opened, until they are closed. I wish I could use library like "pygal" to render the graphs in a browser so that the user can interact with the command line while keeping the figures. 

9. What would you do “next” to expand or augment the project?  
Both family and household incomes were collected along with per income per capita for 2013, but there is no data for those two in other years. The next thing I would do is to collect data for the two variables to cover at least from 2016 to 2018 so that they can be analyzed with  other features including birth rate. Furthermore, the data for home price has limited coverage of US counties (around 800 counties in total). To improve the reliability of this study, I may learn how to decode unicode so that I would be able to scrape more data from PDFs on the internet. Besides the data source, the code can be improved that some functions can be more versatile by being able to deal with various types of requests. For example, the function for generating select statement used in a database  is limited to a select condition of "equal" because the project does not need conditions other than "equal". In fact, the select condition can include "greater than" or "smaller than", which can be extensions for the function to suffice for future demand. I will compare published statistical report from others with my results to confirm my conclusion or generate new findings.    
